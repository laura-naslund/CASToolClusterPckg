# Creates output directory structure (starts at the parent directory of the
#     input directory), where brackets indicate the state abbreviation
#     - out.dir = ClusterOutput/[state]
#         * out.dir/Boundary
#         * out.dir/Ecoregions
#         * out.dir/HCPC
#         * out.dir/Histograms
#         * out.dir/NHDPlus
#         * out.dir/PCA
#         * out.dir/QC
#' @title Reach Classification (generic)
#'
#' @description Generates cluster identifiers for stream reaches within a selected state
#'
#' @details Identifies reach clusters based on stream flow and slope data obtained both from
#'          the nhdplusTools (using an API) and abiotic watershed summary data obtained using StreamCatTools.
#'          If needed, variables are Box-Cox transformed, missing values are imputed, and a PCA performed.
#'          Reaches are then clustered using kmeans on the principal components with a user specified number of clusters.
#'          A hierarchical clustering on the kmeans results is then performed and a final number
#'          of clusters is determined from a user-specified minimum proportion of stream reaches in each cluster.
#'          A final graphic with reach cluster assignments and PCA summary figures is then generated.
#'
#' Uses the libraries tidyverse, nhdplusTools, StreamCatTools, sf, moments, factoextra,
#' cowplot, FactoMineR, missMDA, tmap, viridis, ggrepel, readxl, tictoc
#'
#' @param state The desired state
#' @param qc_keep The EROM variables to be used in the PCA (QC_MM and QC_MA, where
#'                MM refers to the numeric monthly mean estimate and MA refers to
#'                the mean annual flow estimate, which is always required).
#' @param pct_var The minimum percentage variation that the PCA components must explain..
#' @param minCOMIDsCluster The minimum number of COMIDs required to be included in
#'                         the smallest cluster, to ensure that sufficient number
#'                         of sites are considered as comparators. This is expressed
#'                         as a proportion.
#' @param in.dir The path to the input data directory.
#'
#' @return Nothing is returned, but multiple files are written to the output directory.
#'
#' @keywords internal
#'
#' @export
rm(list = ls())
commit_hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
# STEP 1: Prep ----
## set vars ----
tictoc::tic("Total")
state <- "South Carolina"
stateAbb <- state.abb[which(state.name == state)]
yyyymmdd <- format(lubridate::now(), "%Y%m%d")
clusterByEco <- FALSE
qc_keep <- c("QC_04", "QC_05", "QC_08","QC_MA")
pct_var <- 60   # minimum percent of variation explained by components of PCA
minCOMIDsCluster <- 0.2  # percent of COMIDs minimum per cluster, expressed as a proportion
user_numclust <- NULL # user can use this parameter to override the default method of determining the final number of clusters
numkk <- Inf # number of clusters to be used in the k-means pre-processing step of the hierarchical clustering, may be set to any number between 2 and max # reaches; 100 is default
## Declare functions ----
`%>%` <- dplyr::`%>%`
not_all_na <- function(x) {!all(is.na(x))}
source("data-raw/clusterGraphic.R")
source("data-raw/addClusterIDs.R")
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
## Create output directories ----
if(dir.exists(file.path(getwd(), "inst", "extdata", stateAbb)) == FALSE){dir.create(file.path(getwd(), "inst", "extdata", stateAbb))}
if(dir.exists(file.path(getwd(), "data-raw", "ClusterOutput")) == FALSE){dir.create(file.path(getwd(), "data-raw","ClusterOutput"))}
if(dir.exists(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb)) == FALSE){dir.create(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb))}
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
out_folders <- c("Boundary", "Ecoregions", "HCPC", "Histograms", "NHDPlus", "PCA", "QC")
for(i in 1:length(out_folders)){
if(dir.exists(file.path(out.dir, out_folders[i]))==FALSE){
dir.create(file.path(out.dir, out_folders[i]))
}
}
## Load/download required libraries
libs <- c("tidyverse", "nhdplusTools", "StreamCatTools", "sf", "moments", "factoextra",
"cowplot", "FactoMineR", "missMDA", "tmap", "viridis", "ggrepel", "readxl", "tictoc", "usethis")
needed_libs <- setdiff(libs, .packages(all.available = TRUE))
if(rlang::is_empty(needed_libs)==FALSE){
install.packages(needed_libs)}
lapply(libs, require, character.only = TRUE)
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
save(STATE.shp, file = file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda")))
## Get NHD+ data from API ----
# Citation for NHDPlus data: McKay, L., Bondelid, T., Dewald, T., Johnston, J., Moore, R., and Rea, A., “NHDPlus Version 2: User Guide”, 2012 and U.S. Geological Survey, 2019, National Hydrography Dataset (ver. USGS National Hydrography Dataset Best Resolution (NHD) for Hydrologic Unit (HU) [specify number of HuC2s here - 2001 (published 20191002), accessed [date] at https://www.epa.gov/waterdata/get-nhdplus-national-hydrography-dataset-plus-data
# Citation for nhdlusTools: Blodgett, D., Johnson, J.M., 2022, nhdplusTools: Tools for Accessing and Working with the NHDPlus, https://doi.org/10.5066/P97AS8JD
# Desired NHD+ variables
variables <- c("comid", tolower(qc_keep), "slope")
# Check for existence of data
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))) {
message("Previously saved NHDPlus data loaded")
load(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))
} else {
message("Acquiring NHDPlus data")
tictoc::tic("Get NHD+ data")
NHD.STATE <- nhdplusTools::get_nhdplus(AOI = STATE.shp) %>%
dplyr::filter(ftype %in% c("Connector", "CanalDitch", "StreamRiver", "Drainageway", "ArtificialPath"))  %>%
dplyr::select(all_of(variables))
new.names <- c(toupper(variables), "geometry")
colnames(NHD.STATE) <- paste(new.names)
save(NHD.STATE, file = file.path(out.dir, "NHDPlus"
, paste0("NHD_", state, ".rda")))
tictoc::toc(log = TRUE)
}
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# STEP 3: Get StreamCat data ----
## Create a list of StreamCat variables & NHD variables used in the cluster analysis.
## Write a file of StreamCat variables used as stressors in the CASTool.
tictoc::tic("Get StreamCat data")
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda"))) & file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))) {
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda")))
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))
message("Previously saved StreamCat data loaded")
} else {
message("Acquiring StreamCat data")
## Read clustering variables ----
# Citation for StreamCat data: Hill, Ryan A., Marc H. Weber, Scott G. Leibowitz, Anthony R. Olsen, and Darren J. Thornbrugh, 2016. The Stream-Catchment (StreamCat) Dataset: A Database of Watershed Metrics for the Conterminous United States. Journal of the American Water Resources Association (JAWRA) 52:120-128. DOI: 10.1111/1752-1688.12372.
# Citation for StreamCatTools:   Weber, Marc H, Hill, Ryan A., Brookes, Allen F. 2024, StreamCatTools: Tools to work with the StreamCat API within R and access the full suite of StreamCat and LakeCat metrics, https://usepa.github.io/StreamCatTools
sc_ws_metrics <- read_csv(file.path(in.dir, "StreamCat_clusterVars.csv"))  %>%
dplyr::filter(Type == "watershed") %>%
pull(Variable)
sc_ws_metrics_str <- paste(sc_ws_metrics, collapse = ",")
WS.STATE <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
state = stateAbb,
showAreaSqKm = TRUE) %>%
select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_id <- WS.STATE %>% select(COMID) %>%
mutate(ReachLoc = "Core")
stragglers <- setdiff(NHD.STATE$COMID, WS.STATE$COMID)  # COMIDs pulled the from NHD API but not StreamCat API
n_stragglers <- stragglers %>% length()
stragglers_str <- stragglers %>% paste(collapse = ",")
message("Reading in StreamCat variables for straggler COMIDS")
message("Requires ", ceiling(n_stragglers/500), " requests")
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}
}
sc_id <- sc_id %>% bind_rows(sc_stragglers %>% select(COMID) %>% mutate(ReachLoc = "Buffer"))
WS.STATE <- WS.STATE %>% bind_rows(sc_stragglers)
WS.STATE.SCvars <- dplyr::left_join(NHD.STATE, WS.STATE, by = "COMID")
save(WS.STATE.SCvars, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_", state, ".rda")))
save(sc_id, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_ID", state, ".rda")))
}
rlang::last_trace()
View(WS.STATE)
names(WS.STATE)
WS.STATE <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
state = stateAbb,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
# Reach Classification (Generic)
# Naslund.Laura@epa.gov, 20250116
# Ann.RoseberryLincoln@tetratech.com, 20240508
# Based on code written by Tom Barnum, USEPA, 20240229
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# R v4.4.1
# User-specified variables:
#      state name (long form, not abbreviation)
#      qc_keep (a vector of which months' erom estimates to keep, along with mean annual erom estimate)
#      pct_var <- 60   # minimum percent of variation explained by components of PCA used for missing data imputation and hierarchical clustering
#      minCOMIDsCluster <- 0.1  # percent of COMIDs minimum per cluster, expressed as a fraction
# Input data required:
#      gadm41_USA_1 shapefile set of files containing state outline
#      StreamCat_clusterVars.csv containing StreamCat clustering variables
# Required input directory structure
#     - in.dir (contains csv file with potential StreamCat variables to include)
#     - in.dir/gadm41_USA_shp (contains USA state shapefiles)
# Creates output directory structure (starts at the parent directory of the
#     input directory), where brackets indicate the state abbreviation
#     - out.dir = ClusterOutput/[state]
#         * out.dir/Boundary
#         * out.dir/Ecoregions
#         * out.dir/HCPC
#         * out.dir/Histograms
#         * out.dir/NHDPlus
#         * out.dir/PCA
#         * out.dir/QC
#' @title Reach Classification (generic)
#'
#' @description Generates cluster identifiers for stream reaches within a selected state
#'
#' @details Identifies reach clusters based on stream flow and slope data obtained both from
#'          the nhdplusTools (using an API) and abiotic watershed summary data obtained using StreamCatTools.
#'          If needed, variables are Box-Cox transformed, missing values are imputed, and a PCA performed.
#'          Reaches are then clustered using kmeans on the principal components with a user specified number of clusters.
#'          A hierarchical clustering on the kmeans results is then performed and a final number
#'          of clusters is determined from a user-specified minimum proportion of stream reaches in each cluster.
#'          A final graphic with reach cluster assignments and PCA summary figures is then generated.
#'
#' Uses the libraries tidyverse, nhdplusTools, StreamCatTools, sf, moments, factoextra,
#' cowplot, FactoMineR, missMDA, tmap, viridis, ggrepel, readxl, tictoc
#'
#' @param state The desired state
#' @param qc_keep The EROM variables to be used in the PCA (QC_MM and QC_MA, where
#'                MM refers to the numeric monthly mean estimate and MA refers to
#'                the mean annual flow estimate, which is always required).
#' @param pct_var The minimum percentage variation that the PCA components must explain..
#' @param minCOMIDsCluster The minimum number of COMIDs required to be included in
#'                         the smallest cluster, to ensure that sufficient number
#'                         of sites are considered as comparators. This is expressed
#'                         as a proportion.
#' @param in.dir The path to the input data directory.
#'
#' @return Nothing is returned, but multiple files are written to the output directory.
#'
#' @keywords internal
#'
#' @export
rm(list = ls())
commit_hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
# STEP 1: Prep ----
## set vars ----
tictoc::tic("Total")
state <- "South Carolina"
stateAbb <- state.abb[which(state.name == state)]
yyyymmdd <- format(lubridate::now(), "%Y%m%d")
clusterByEco <- FALSE
qc_keep <- c("QC_04", "QC_05", "QC_08","QC_MA")
pct_var <- 60   # minimum percent of variation explained by components of PCA
minCOMIDsCluster <- 0.2  # percent of COMIDs minimum per cluster, expressed as a proportion
user_numclust <- NULL # user can use this parameter to override the default method of determining the final number of clusters
numkk <- Inf # number of clusters to be used in the k-means pre-processing step of the hierarchical clustering, may be set to any number between 2 and max # reaches; 100 is default
## Declare functions ----
`%>%` <- dplyr::`%>%`
not_all_na <- function(x) {!all(is.na(x))}
source("data-raw/clusterGraphic.R")
source("data-raw/addClusterIDs.R")
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
## Create output directories ----
if(dir.exists(file.path(getwd(), "inst", "extdata", stateAbb)) == FALSE){dir.create(file.path(getwd(), "inst", "extdata", stateAbb))}
if(dir.exists(file.path(getwd(), "data-raw", "ClusterOutput")) == FALSE){dir.create(file.path(getwd(), "data-raw","ClusterOutput"))}
if(dir.exists(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb)) == FALSE){dir.create(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb))}
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
out_folders <- c("Boundary", "Ecoregions", "HCPC", "Histograms", "NHDPlus", "PCA", "QC")
for(i in 1:length(out_folders)){
if(dir.exists(file.path(out.dir, out_folders[i]))==FALSE){
dir.create(file.path(out.dir, out_folders[i]))
}
}
## Load/download required libraries
libs <- c("tidyverse", "nhdplusTools", "StreamCatTools", "sf", "moments", "factoextra",
"cowplot", "FactoMineR", "missMDA", "tmap", "viridis", "ggrepel", "readxl", "tictoc", "usethis")
needed_libs <- setdiff(libs, .packages(all.available = TRUE))
if(rlang::is_empty(needed_libs)==FALSE){
install.packages(needed_libs)}
lapply(libs, require, character.only = TRUE)
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
save(STATE.shp, file = file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda")))
## Get NHD+ data from API ----
# Citation for NHDPlus data: McKay, L., Bondelid, T., Dewald, T., Johnston, J., Moore, R., and Rea, A., “NHDPlus Version 2: User Guide”, 2012 and U.S. Geological Survey, 2019, National Hydrography Dataset (ver. USGS National Hydrography Dataset Best Resolution (NHD) for Hydrologic Unit (HU) [specify number of HuC2s here - 2001 (published 20191002), accessed [date] at https://www.epa.gov/waterdata/get-nhdplus-national-hydrography-dataset-plus-data
# Citation for nhdlusTools: Blodgett, D., Johnson, J.M., 2022, nhdplusTools: Tools for Accessing and Working with the NHDPlus, https://doi.org/10.5066/P97AS8JD
# Desired NHD+ variables
variables <- c("comid", tolower(qc_keep), "slope")
# Check for existence of data
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))) {
message("Previously saved NHDPlus data loaded")
load(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))
} else {
message("Acquiring NHDPlus data")
tictoc::tic("Get NHD+ data")
NHD.STATE <- nhdplusTools::get_nhdplus(AOI = STATE.shp) %>%
dplyr::filter(ftype %in% c("Connector", "CanalDitch", "StreamRiver", "Drainageway", "ArtificialPath"))  %>%
dplyr::select(all_of(variables))
new.names <- c(toupper(variables), "geometry")
colnames(NHD.STATE) <- paste(new.names)
save(NHD.STATE, file = file.path(out.dir, "NHDPlus"
, paste0("NHD_", state, ".rda")))
tictoc::toc(log = TRUE)
}
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# STEP 3: Get StreamCat data ----
## Create a list of StreamCat variables & NHD variables used in the cluster analysis.
## Write a file of StreamCat variables used as stressors in the CASTool.
tictoc::tic("Get StreamCat data")
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda"))) & file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))) {
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda")))
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))
message("Previously saved StreamCat data loaded")
} else {
message("Acquiring StreamCat data")
## Read clustering variables ----
# Citation for StreamCat data: Hill, Ryan A., Marc H. Weber, Scott G. Leibowitz, Anthony R. Olsen, and Darren J. Thornbrugh, 2016. The Stream-Catchment (StreamCat) Dataset: A Database of Watershed Metrics for the Conterminous United States. Journal of the American Water Resources Association (JAWRA) 52:120-128. DOI: 10.1111/1752-1688.12372.
# Citation for StreamCatTools:   Weber, Marc H, Hill, Ryan A., Brookes, Allen F. 2024, StreamCatTools: Tools to work with the StreamCat API within R and access the full suite of StreamCat and LakeCat metrics, https://usepa.github.io/StreamCatTools
sc_ws_metrics <- read_csv(file.path(in.dir, "StreamCat_clusterVars.csv"))  %>%
dplyr::filter(Type == "watershed") %>%
pull(Variable)
sc_ws_metrics_str <- paste(sc_ws_metrics, collapse = ",")
WS.STATE <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
state = stateAbb,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_id <- WS.STATE %>% select(COMID) %>%
mutate(ReachLoc = "Core")
stragglers <- setdiff(NHD.STATE$COMID, WS.STATE$COMID)  # COMIDs pulled the from NHD API but not StreamCat API
n_stragglers <- stragglers %>% length()
stragglers_str <- stragglers %>% paste(collapse = ",")
message("Reading in StreamCat variables for straggler COMIDS")
message("Requires ", ceiling(n_stragglers/500), " requests")
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}
}
sc_id <- sc_id %>% bind_rows(sc_stragglers %>% select(COMID) %>% mutate(ReachLoc = "Buffer"))
WS.STATE <- WS.STATE %>% bind_rows(sc_stragglers)
WS.STATE.SCvars <- dplyr::left_join(NHD.STATE, WS.STATE, by = "COMID")
save(WS.STATE.SCvars, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_", state, ".rda")))
save(sc_id, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_ID", state, ".rda")))
}
rlang::last_trace()
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}
}
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
#dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}
}
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
#dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
temp.comids
q <- 1
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp.comids
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
ceiling(n_stragglers/500)
q <- 4
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind] %>% paste(collapse = ",")
temp.comids
stragglers[start_ind:end_ind]
class(stragglers[start_ind:end_ind])
temp.comids <- stragglers[start_ind:end_ind]
temp.comids[!is.na(temp.comids)]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
temp.comids
q <- 2
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp.comids
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
q <- 3
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
q <- 4
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
rlang::last_trace()
rlang::last_trace(drop = FALSE)
temp.comids
temp.comids <- 2
q <- 3
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp.comids
temp.comids2
q <- 4
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
temp.comids
StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = "11561978",
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = "11561988",
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
install_github("USEPA/StreamCatTools", build_vignettes=FALSE)
library(remotes)
install_github("USEPA/StreamCatTools", build_vignettes=FALSE)
3
