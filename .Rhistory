st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
save(STATE.shp, file = file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda")))
## Get NHD+ data from API ----
# Citation for NHDPlus data: McKay, L., Bondelid, T., Dewald, T., Johnston, J., Moore, R., and Rea, A., “NHDPlus Version 2: User Guide”, 2012 and U.S. Geological Survey, 2019, National Hydrography Dataset (ver. USGS National Hydrography Dataset Best Resolution (NHD) for Hydrologic Unit (HU) [specify number of HuC2s here - 2001 (published 20191002), accessed [date] at https://www.epa.gov/waterdata/get-nhdplus-national-hydrography-dataset-plus-data
# Citation for nhdlusTools: Blodgett, D., Johnson, J.M., 2022, nhdplusTools: Tools for Accessing and Working with the NHDPlus, https://doi.org/10.5066/P97AS8JD
# Desired NHD+ variables
variables <- c("comid", tolower(qc_keep), "slope")
# Check for existence of data
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))) {
message("Previously saved NHDPlus data loaded")
load(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))
} else {
message("Acquiring NHDPlus data")
tictoc::tic("Get NHD+ data")
NHD.STATE <- nhdplusTools::get_nhdplus(AOI = STATE.shp) %>%
dplyr::filter(ftype %in% c("Connector", "CanalDitch", "StreamRiver", "Drainageway", "ArtificialPath"))  %>%
dplyr::select(all_of(variables))
new.names <- c(toupper(variables), "geometry")
colnames(NHD.STATE) <- paste(new.names)
save(NHD.STATE, file = file.path(out.dir, "NHDPlus"
, paste0("NHD_", state, ".rda")))
tictoc::toc(log = TRUE)
}
# STEP 3: Get StreamCat data ----
## Create a list of StreamCat variables & NHD variables used in the cluster analysis.
## Write a file of StreamCat variables used as stressors in the CASTool.
tictoc::tic("Get StreamCat data")
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda"))) & file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))) {
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda")))
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))
message("Previously saved StreamCat data loaded")
} else {
message("Acquiring StreamCat data")
## Read clustering variables ----
# Citation for StreamCat data: Hill, Ryan A., Marc H. Weber, Scott G. Leibowitz, Anthony R. Olsen, and Darren J. Thornbrugh, 2016. The Stream-Catchment (StreamCat) Dataset: A Database of Watershed Metrics for the Conterminous United States. Journal of the American Water Resources Association (JAWRA) 52:120-128. DOI: 10.1111/1752-1688.12372.
# Citation for StreamCatTools:   Weber, Marc H, Hill, Ryan A., Brookes, Allen F. 2024, StreamCatTools: Tools to work with the StreamCat API within R and access the full suite of StreamCat and LakeCat metrics, https://usepa.github.io/StreamCatTools
sc_ws_metrics <- read_csv(file.path(in.dir, "StreamCat_clusterVars.csv"))  %>%
dplyr::filter(Type == "watershed") %>%
pull(Variable)
sc_ws_metrics_str <- paste(sc_ws_metrics, collapse = ",")
WS.STATE <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
state = stateAbb,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_id <- WS.STATE %>% dplyr::select(COMID) %>%
mutate(ReachLoc = "Core")
stragglers <- setdiff(NHD.STATE$COMID, WS.STATE$COMID)  # COMIDs pulled the from NHD API but not StreamCat API
n_stragglers <- stragglers %>% length()
stragglers_str <- stragglers %>% paste(collapse = ",")
message("Reading in StreamCat variables for straggler COMIDS")
message("Requires ", ceiling(n_stragglers/500), " requests")
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
tryCatch({temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}, error = function(msg){
return(sc_stragglers)
})
}
}
sc_id <- sc_id %>% bind_rows(sc_stragglers %>% dplyr::select(COMID) %>% mutate(ReachLoc = "Buffer"))
WS.STATE <- WS.STATE %>% bind_rows(sc_stragglers)
WS.STATE.SCvars <- dplyr::left_join(NHD.STATE, WS.STATE, by = "COMID")
save(WS.STATE.SCvars, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_", state, ".rda")))
save(sc_id, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_ID", state, ".rda")))
}
tictoc::toc(log = TRUE)
# STEP 4: Data QC ----
tictoc::tic("Perform QC of stream reach data")
# check if there are any exact duplicates and remove
n_dup <- WS.STATE.SCvars %>% group_by_all() %>% filter(n()>1) %>% nrow()
if(n_dup == 0){
message("No duplicated rows")
}
if(n_dup!= 0){
message("Removing duplicate rows")
WS.STATE.SCvars <- WS.STATE.SCvars %>% distinct()
}
# drop columns with all NAs
na_cols <- WS.STATE.SCvars %>%
st_drop_geometry() %>%
select_if(function(x) all(is.na(x))) %>%
names()
if(length(na_cols) == 0){
message("No completely empty columns")
}
if(length(na_cols) != 0){
WS.STATE.SCvars <- WS.STATE.SCvars %>% dplyr::select(-all_of(na_cols))
}
# replace negative slopes with NAs
n_neg_slope <- WS.STATE.SCvars %>% filter(SLOPE < 0) %>% nrow()
WS.STATE.SCvars <- WS.STATE.SCvars %>% mutate(SLOPE = if_else(SLOPE < 0, NA, SLOPE))
message(paste0("Replacing ", n_neg_slope, " negative slope values with NAs"))
# remove COMIDs without any StreamCat data
WS.STATE.FinalRaw <- WS.STATE.SCvars %>%
st_drop_geometry() %>%
rename("wsareasqkmws" = "wsareasqkm") %>%
filter(!(if_all(ends_with("ws"), is.na)))
n_na_row <- WS.STATE.SCvars %>% rename("wsareasqkmws" = "wsareasqkm") %>% filter(if_all(ends_with("ws"), is.na)) %>% nrow()
message(paste0("Removing ", n_na_row, " rows without StreamCat data"))
save(WS.STATE.FinalRaw, file = file.path(out.dir, "PCA",
paste0("FinalRawData", state, ".rda")))
tictoc::toc(log = TRUE)
# STEP 5: Finalize dataset ----
## Get stats ----
tictoc::tic("Get statistics & transform variables, if necessary")
if (file.exists(file.path(out.dir, "Histograms", paste0(state, "_stats.tab")))) {
WS.STATE.stats <- read.delim(file.path(out.dir, "Histograms", paste0(state, "_stats.tab")) , sep = "\t")
message("Previously saved histogram stats")
} else {
WS.STATE.stats <- WS.STATE.FinalRaw %>%
tidyr::pivot_longer(cols = !COMID, names_to = "Variable", values_to = "Value",
values_drop_na = FALSE) %>%
dplyr::group_by(Variable) %>%
dplyr::summarise(N = dplyr::n() - sum(is.na(Value))
, NumNAs = sum(is.na(Value))
, Min = min(Value, na.rm = TRUE)
, Max = max(Value, na.rm = TRUE)
, Mean = mean(Value, na.rm = TRUE)
, Median = median(Value, na.rm = TRUE)
, SD = sd(Value, na.rm = TRUE)
, Kurtosis = moments::kurtosis(Value, na.rm = TRUE)
, Skewness = moments::skewness(Value, na.rm = TRUE)
, SkewnessSq = Skewness * Skewness
, .groups = "drop_last")
write.table(WS.STATE.stats
, file.path(out.dir, "Histograms", paste0(state, "_stats.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
}
if (file.exists(file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab"))) & file.exists(file.path(out.dir, "QC", paste0(state, "_lambdas.tab")))){
WS.STATE.FinalTransf <- read.delim(file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab")), sep = "\t")
df.lambda <- read.delim(file.path(out.dir, "QC", paste0(state, "_lambdas.tab")), sep = "\t")
message("Previously saved QC files")
} else{
## Transform/scale variables ----
## Plot histograms before/after ----
cols <- setdiff(colnames(WS.STATE.FinalRaw), "COMID")
df.temp <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.temp_scaled <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.lambda <- data.frame(Variable = character(), Lambda = double())
rm_vars <- c()
for (c in seq_along(cols)) {
col <- cols[c]
print(col)
fn <- paste0(col, ".png")
fn2 <- paste0(col, "_transf.png")
# plot histogram of data
p <- ggplot2::ggplot(WS.STATE.FinalRaw, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn), p, width = 6, height = 4
, units = "in")
sk_sq <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SkewnessSq)
sd_dat <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SD)
if (as.numeric(sd_dat) != 0) {
if (grepl("PCT", col)) {         # Do not transform PCT variables
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else if (as.numeric(sk_sq)< 3){                                # Do not transform values with minimal skewness
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else {     # Box-Cox transform highly skewed variables
v_val <- WS.STATE.FinalRaw[[col]] + 1e-12
# browser()
bc <- caret::BoxCoxTrans(v_val, na.rm = TRUE)
lambda <- bc$lambda
# bc <- MASS::boxcox(lm(v_val ~ 1), plotit = FALSE) # for some reason, MASS::boxcox will not evaluate in a function
# lambda <- bc$x[which.max(bc$y)]
if (lambda != 0) {
new_v <- ((v_val ^ lambda) - 1) / lambda
} else {
msg <- paste(col, " has lambda equal zero.")
message(msg)
new_v <- log10(v_val)
}
if (all(is.na(new_v))) {
message("all new_v NA")
new_v <- WS.STATE.FinalRaw[[col]]
subtitle <- "Not transformed, centered and scaled"
} else {
subtitle <- paste0("lambda = ", round(lambda, 4), ", centered and scaled")
}
df.temp <- df.temp %>% mutate(!!col := new_v)
}
# Scale variables, this is just for visualization of the centered and scaled distribution.
# The the unscaled and uncentered data are passed to the PCA functions which scale and center the data.
new_v_scaled <- scale(new_v, center = TRUE, scale = TRUE)
df.temp_scaled <- df.temp_scaled %>% mutate(!!col := new_v_scaled)
p2 <- ggplot2::ggplot(df.temp_scaled, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::labs(subtitle = subtitle) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn2), p2
, width = 6, height = 4, units = "in")
df.lambda <- rbind(df.lambda, cbind(col, round(lambda, 4)))
}
if (as.numeric(sd_dat) == 0) {          # Do not include variables with zero variation
rm_vars <- c(rm_vars, col)
}
}
message(paste0("removed: ", paste(rm_vars, collapse = ", "), " for 0 variation"))
WS.STATE.FinalTransf <- df.temp
write.table(WS.STATE.FinalTransf, file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
write.table(df.lambda, file.path(out.dir, "QC", paste0(state, "_lambdas.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
}
cols
## Transform/scale variables ----
## Plot histograms before/after ----
cols <- setdiff(colnames(WS.STATE.FinalRaw), "COMID")
df.temp <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.temp_scaled <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.lambda <- data.frame(Variable = character(), Lambda = double())
rm_vars <- c()
for (c in seq_along(cols)) {
col <- cols[c]
print(col)
fn <- paste0(col, ".png")
fn2 <- paste0(col, "_transf.png")
# plot histogram of data
p <- ggplot2::ggplot(WS.STATE.FinalRaw, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn), p, width = 6, height = 4
, units = "in")
sk_sq <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SkewnessSq)
sd_dat <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SD)
if (as.numeric(sd_dat) != 0) {
if (grepl("pct", col)) {         # Do not transform PCT variables
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else if (as.numeric(sk_sq)< 3){                                # Do not transform values with minimal skewness
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else {     # Box-Cox transform highly skewed variables
v_val <- WS.STATE.FinalRaw[[col]] + 1e-12
# browser()
bc <- caret::BoxCoxTrans(v_val, na.rm = TRUE)
lambda <- bc$lambda
# bc <- MASS::boxcox(lm(v_val ~ 1), plotit = FALSE) # for some reason, MASS::boxcox will not evaluate in a function
# lambda <- bc$x[which.max(bc$y)]
if (lambda != 0) {
new_v <- ((v_val ^ lambda) - 1) / lambda
} else {
msg <- paste(col, " has lambda equal zero.")
message(msg)
new_v <- log10(v_val)
}
if (all(is.na(new_v))) {
message("all new_v NA")
new_v <- WS.STATE.FinalRaw[[col]]
subtitle <- "Not transformed, centered and scaled"
} else {
subtitle <- paste0("lambda = ", round(lambda, 4), ", centered and scaled")
}
df.temp <- df.temp %>% mutate(!!col := new_v)
}
# Scale variables, this is just for visualization of the centered and scaled distribution.
# The the unscaled and uncentered data are passed to the PCA functions which scale and center the data.
new_v_scaled <- scale(new_v, center = TRUE, scale = TRUE)
df.temp_scaled <- df.temp_scaled %>% mutate(!!col := new_v_scaled)
p2 <- ggplot2::ggplot(df.temp_scaled, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::labs(subtitle = subtitle) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn2), p2
, width = 6, height = 4, units = "in")
df.lambda <- rbind(df.lambda, cbind(col, round(lambda, 4)))
}
if (as.numeric(sd_dat) == 0) {          # Do not include variables with zero variation
rm_vars <- c(rm_vars, col)
}
}
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state.vec <- c("Oregon", "Washington", "Delaware")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
write.csv(df, file.path("inst", "extdata", "pick_list.csv"), row.names = FALSE)
load("~/3-projects/10-castool/CASToolClusterPckg/data/Washington_ClusterAssignments_20250714115415_4_0f5f4dfb.rda")
View(Washington_ClusterAssignments_20250714115415_4_0f5f4dfb)
check()
devtools::check()
devtools::install()
use_import_from("stringr", "str_replace")
library(devtools)
use_import_from("stringr", "str_replace")
check()
boundary_fp <- file.path(out.dir, "Boundary", paste0("Delaware", "_BoundaryShapefile.rda"))
state <- "Connecticut"
stateAbb <- "CT"
file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
file.copy(boundary_fp, file.path("inst", "extdata", stateAbb, paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE)
boundary_fp
state <- "Delaware"
stateAbb <- "DE"
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
file.copy(boundary_fp, file.path("inst", "extdata", stateAbb, paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE)
state <- "Oregon"
stateAbb <- "OR"
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
file.copy(boundary_fp, file.path("inst", "extdata", stateAbb, paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE)
state <- "Washington"
stateAbb <- "WA"
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
file.copy(boundary_fp, file.path("inst", "extdata", stateAbb, paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE)
state.vec <- c("Connecticut")
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
state.vec <- c("Connecticut")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
load_all()
data(list = system.file("extdata", "CT", "Connecticut_BoundaryShapefile"))
load("~/3-projects/10-castool/CASToolClusterPckg/inst/extdata/CT/Connecticut_BoundaryShapefile.rda")
use_data("Connecticut_BoundaryShapefile")
Connecticut_BoundaryShapefile <- load("~/3-projects/10-castool/CASToolClusterPckg/inst/extdata/CT/Connecticut_BoundaryShapefile.rda")
plot(Connecticut_BoundaryShapefile)
mapview::mapview(Connecticut_BoundaryShapefile)
state <- "Connecticut"
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
# file.copy(boundary_fp, file.path("inst", "extdata", stateAbb, paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE)
assign(paste0(state, "_BoundaryShapefile.rda"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile.rda")), overwrite = TRUE))
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
use_r("retrieve_boundary")
load_all()
temp <- retrieve_boundary("Connecticut")
load("~/3-projects/10-castool/CASToolClusterPckg/data/Connecticut_BoundaryShapefile.rda")
str(Connecticut_BoundaryShapefile)
mapview::mapview(Connecticut_BoundaryShapefile)
load("~/3-projects/10-castool/CASToolClusterPckg/data-raw/ClusterOutput/CT/Boundary/Connecticut_BoundaryShapefile.rda")
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
state <- "Connecticut"
stateAbb <- "CT"
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
mapview::mapview(STATE.shp)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
save(STATE.shp, file = boundary_fp)
assign(paste0(state, "_BoundaryShapefile.rda"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
install()
data("Connecticut_BoundaryShapefile", package = "CASToolClusterPckg")
View(STATE.shp)
temp <- retrieve_boundary("Connecticut")
temp <- retrieve_boundary("Connecticut")
load_all()
temp <- retrieve_boundary("Connecticut")
temp
temp <- retrieve_boundary("Connecticut")
boundary_name <- paste(state, "BoundaryShapefile", sep = "_")
state <- "Connecticut"
boundary_name <- paste(state, "BoundaryShapefile", sep = "_")
data(list = boundary_name, package = "CASToolClusterPckg", envir = environment())
boundary_name
data(list = boundary_name, package = "CASToolClusterPckg", envir = environment())
data("Connecticut_BoundaryShapefile", package = "CASToolClusterPckg")
load("~/3-projects/10-castool/CASToolClusterPckg/data/Connecticut_BoundaryShapefile.rda")
load("~/3-projects/10-castool/CASToolClusterPckg/data/Connecticut_BoundaryShapefile.rda")
load("~/3-projects/10-castool/CASToolClusterPckg/inst/extdata/CT/Connecticut_BoundaryShapefile.rda")
str(STATE.shp)
class(STATE.shp)
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state <- "Connecticut"
stateAbb <- "CT"
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
save(STATE.shp, file = boundary_fp)
assign(paste0(state, "_BoundaryShapefile"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
retrieve_boundary("Connecticut")
temp <- retrieve_boundary("Connecticut")
mapview::mapview(temp)
state <- "Oregon"
stateAbb <- "OR"
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
save(STATE.shp, file = boundary_fp)
assign(paste0(state, "_BoundaryShapefile"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
state <- "Washington"
stateAbb <- "WA"
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
save(STATE.shp, file = boundary_fp)
assign(paste0(state, "_BoundaryShapefile"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
state <- "Delaware"
stateAbb <- "DE"
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
boundary_fp <- file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda"))
save(STATE.shp, file = boundary_fp)
assign(paste0(state, "_BoundaryShapefile"), STATE.shp)
do.call("use_data", list(as.name(paste0(state, "_BoundaryShapefile")), overwrite = TRUE))
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state.vec <- c("Connecticut")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
devtools::check()
