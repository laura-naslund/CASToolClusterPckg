devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut", 3)
View(clust_df)
clust_df %>% group_by(ClusterID) %>% summarize(count = n())
clust_df %>% dplyr::group_by(ClusterID) %>% dplyr::summarize(count = n())
clust_df %>% dplyr::group_by(ClusterID) %>% dplyr::summarize(count = dplyr::n())
View(pick_list)
devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut")
View(pick_list)
clust_df <- retrieve_clust_data("Connecticut")
devtools::load_all
devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut", 4)
clust_df <- retrieve_clust_data("Connecticut", "default")
names(data())
# Use data() to list datasets in the package
datasetInfo <- data(package = "CASToolClusterPckg")
# Extract dataset names
datasetNames <- datasetInfo$results[, "Item"]
intersect("Connecticut_ClusterAssignments_20250710170312_default_2_b9a77bec", datasetNames)
devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut", "default")
nchar(Connecticut_ClusterAssignments_20250710170312_default_2_b9a77bec)
nchar("Connecticut_ClusterAssignments_20250710170312_default_2_b9a77bec")
pick_list %>% dplyr::filter(state == "Connecticut", numclust == as.character("default")) %>% dplyr::pull(fn)
filePath <- system.file("extdata", "pick_list.csv", package = "CASToolClusterPckg")
pick_list <- read.csv(filePath)
pick_list %>% dplyr::filter(state == "Connecticut", numclust == as.character("default")) %>% dplyr::pull(fn)
ret_data_file <- pick_list %>% dplyr::filter(state == "Connecticut", numclust == as.character("default")) %>% dplyr::pull(fn)
ret_data <- get(ret_data_file, envir = environment())
ret_data_file <- pick_list %>% dplyr::filter(state == "Rhode Island", numclust == as.character("default")) %>% dplyr::pull(fn)
data(list = ret_data_file, package = "CASToolClusterPckg", envir = environment())
ret_data <- get(ret_data_file, envir = environment())
pick_list %>% dplyr::filter(state == "Rhode Island", numclust == as.character("default"))
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state.vec <- c("Rhode Island", "Connecticut", "Delaware")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
write.csv(df, file.path("inst", "extdata", "pick_list.csv"), row.names = FALSE)
write.csv(df, file.path("inst", "extdata", "pick_list.csv"), row.names = FALSE)
devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut", "default")
View(clust_df)
clust_df <- retrieve_clust_data("North Carolina", 2)
filePath <- system.file("extdata", "pick_list.csv", package = "CASToolClusterPckg")
pick_list <- read.csv(filePath)
pick_list %>% dplyr::filter(state == "North Carolina")
pick_list %>% dplyr::filter(state == "North Carolina") %>% pull(fn)
devtools::load_all()
clust_df <- retrieve_clust_data("North Carolina", 2)
State <- "North Carolina"
Clust_num <- 3
filePath <- system.file("extdata", "pick_list.csv", package = "CASToolClusterPckg")
pick_list <- read.csv(filePath)
ret_data_file <- pick_list %>% dplyr::filter(state == State, numclust == as.character(Clust_num)) %>% dplyr::pull(fn)
if(length(ret_data_file)==0){
ret_data <- NULL
message("No data available with requested data and cluster number")
} else{
data(list = ret_data_file, package = "CASToolClusterPckg", envir = environment())
ret_data <- get(ret_data_file, envir = environment())
}
devtools::load_all()
clust_df <- retrieve_clust_data("Connecticut", "default")
clust_df <- retrieve_clust_data("North Carolina", 2)
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
use_r("retrieve_clust_fig")
fp <- system.file("extdata", "Connecticut_ClusterGraphics_20250714103604_2_0f5f4dfb.png", package = "CASToolClusterPckg")
devtools::load_all()
fp <- system.file("extdata", "Connecticut_ClusterGraphics_20250714103604_2_0f5f4dfb.png", package = "CASToolClusterPckg")
fp <- system.file("extdata", "CT", "Connecticut_ClusterGraphics_20250714103604_2_0f5f4dfb.png", package = "CASToolClusterPckg")
print(fp)
im <- load.image(fp)
im <- imager::load.image(fp)
print(im)
fp <- system.file("extdata", "CT", "Connecticut_ClusterGraphics_20250714103604_2_0f5f4dfb.png", package = "CASToolClusterPckg")
test_img <- magick::image_read(fp)
test_img
head(pick_list)
ret_data_file <- pick_list %>% dplyr::filter(state == State, numclust == as.character(Clust_num)) %>% dplyr::pull(fn)
filePath <- system.file("extdata", "pick_list.csv", package = "CASToolClusterPckg")
pick_list <- read.csv(filePath)
head(pick_list)
devtools::load_all()
test_fp <- retrieve_clust_fig("Connecticut", 2)
test_img <- magick::image_read(fp)
test_img
devtools::load_all()
test_fp <- retrieve_clust_fig("Connecticut", 2)
test_fp
State <- "Connecticut"
Clust_num <- 2
ret_fp <- system.file("extdata", state.abb[which(state.name == State)], str_replace(ret_data_file, "Assignments", "Graphics"), package = "CASToolClusterPckg")
ret_data_file <- pick_list %>% dplyr::filter(state == State, numclust == as.character(Clust_num)) %>% dplyr::pull(fn)
ret_fp <- system.file("extdata", state.abb[which(state.name == State)], str_replace(ret_data_file, "Assignments", "Graphics"), package = "CASToolClusterPckg")
ret_fp
state.abb[which(state.name == State)]
str_replace(ret_data_file, "Assignments", "Graphics")
paste0(str_replace(ret_data_file, "Assignments", "Graphics"), ".png")
ret_fp <- system.file("extdata", state.abb[which(state.name == State)], paste0(str_replace(ret_data_file, "Assignments", "Graphics"), ".png"), package = "CASToolClusterPckg")
ret_fp
devtools::load_all()
test_fp <- retrieve_clust_fig("Connecticut", 2)
test_fp
test_img <- magick::image_read(fp)
test_img
test_fp <- retrieve_clust_fig("Connecticut", 4)
test_img <- magick::image_read(fp)
test_img
test_fp <- retrieve_clust_fig("Connecticut", 4)
test_img <- magick::image_read(test_fp)
test_img
retrive_clust_fig("Connecticut", 7)
retrieve_clust_fig("Connecticut", 7)
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state.vec <- c("Oregon", "Washington", "Delaware")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
traceback()
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state <- "Washington"
pct_var <- 60
minCOMIDsCluster <- 0.2
user_numclust <- NULL
stateAbb <- state.abb[which(state.name == state)]
yyyymmdd <- format(lubridate::now(), "%Y%m%d")
clusterByEco <- FALSE
qc_keep <- c("QC_04", "QC_05", "QC_08","QC_MA")
## Declare functions ----
`%>%` <- dplyr::`%>%`
not_all_na <- function(x) {!all(is.na(x))}
source("data-raw/clusterGraphic.R")
source("data-raw/addClusterIDs.R")
## Set input directory
in.dir <- file.path(getwd(), "data-raw", "ClusterInput")
## Create output directories ----
if(dir.exists(file.path(getwd(), "inst", "extdata", stateAbb)) == FALSE){dir.create(file.path(getwd(), "inst", "extdata", stateAbb))}
if(dir.exists(file.path(getwd(), "data-raw", "ClusterOutput")) == FALSE){dir.create(file.path(getwd(), "data-raw","ClusterOutput"))}
if(dir.exists(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb)) == FALSE){dir.create(file.path(getwd(),"data-raw", "ClusterOutput", stateAbb))}
out.dir <- file.path(getwd(), "data-raw","ClusterOutput", stateAbb)
out_folders <- c("Boundary", "HCPC", "Histograms", "NHDPlus", "PCA", "QC")
for(i in 1:length(out_folders)){
if(dir.exists(file.path(out.dir, out_folders[i]))==FALSE){
dir.create(file.path(out.dir, out_folders[i]))
}
}
## Load/download required libraries
libs <- c("tidyverse", "nhdplusTools", "StreamCatTools", "sf", "moments", "factoextra",
"cowplot", "FactoMineR", "missMDA", "tmap", "viridis", "ggrepel", "readxl", "tictoc", "usethis", "caret")
needed_libs <- setdiff(libs, .packages(all.available = TRUE))
if(rlang::is_empty(needed_libs)==FALSE){
install.packages(needed_libs)}
lapply(libs, require, character.only = TRUE)
# STEP 2: Get NHD+ data ----
## Get state boundaries ----
# downloaded GADM from https://gadm.org/
STATE.shp <- sf::read_sf(file.path(in.dir,"gadm41_USA_shp/gadm41_USA_1.shp")) %>% filter(NAME_1 == state) %>%
st_transform(crs = 5070) %>% st_buffer(300) # add 300 m buffer to deal with simplified input shapefile geometry
save(STATE.shp, file = file.path(out.dir, "Boundary", paste0(state, "_BoundaryShapefile.rda")))
## Get NHD+ data from API ----
# Citation for NHDPlus data: McKay, L., Bondelid, T., Dewald, T., Johnston, J., Moore, R., and Rea, A., “NHDPlus Version 2: User Guide”, 2012 and U.S. Geological Survey, 2019, National Hydrography Dataset (ver. USGS National Hydrography Dataset Best Resolution (NHD) for Hydrologic Unit (HU) [specify number of HuC2s here - 2001 (published 20191002), accessed [date] at https://www.epa.gov/waterdata/get-nhdplus-national-hydrography-dataset-plus-data
# Citation for nhdlusTools: Blodgett, D., Johnson, J.M., 2022, nhdplusTools: Tools for Accessing and Working with the NHDPlus, https://doi.org/10.5066/P97AS8JD
# Desired NHD+ variables
variables <- c("comid", tolower(qc_keep), "slope")
# Check for existence of data
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))) {
message("Previously saved NHDPlus data loaded")
load(file.path(out.dir, "NHDPlus", paste0("NHD_", state, ".rda")))
} else {
message("Acquiring NHDPlus data")
tictoc::tic("Get NHD+ data")
NHD.STATE <- nhdplusTools::get_nhdplus(AOI = STATE.shp) %>%
dplyr::filter(ftype %in% c("Connector", "CanalDitch", "StreamRiver", "Drainageway", "ArtificialPath"))  %>%
dplyr::select(all_of(variables))
new.names <- c(toupper(variables), "geometry")
colnames(NHD.STATE) <- paste(new.names)
save(NHD.STATE, file = file.path(out.dir, "NHDPlus"
, paste0("NHD_", state, ".rda")))
tictoc::toc(log = TRUE)
}
# STEP 3: Get StreamCat data ----
## Create a list of StreamCat variables & NHD variables used in the cluster analysis.
## Write a file of StreamCat variables used as stressors in the CASTool.
tictoc::tic("Get StreamCat data")
if (file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda"))) & file.exists(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))) {
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_", state, ".rda")))
load(file.path(out.dir, "NHDPlus", paste0("NHD_SC_ID", state, ".rda")))
message("Previously saved StreamCat data loaded")
} else {
message("Acquiring StreamCat data")
## Read clustering variables ----
# Citation for StreamCat data: Hill, Ryan A., Marc H. Weber, Scott G. Leibowitz, Anthony R. Olsen, and Darren J. Thornbrugh, 2016. The Stream-Catchment (StreamCat) Dataset: A Database of Watershed Metrics for the Conterminous United States. Journal of the American Water Resources Association (JAWRA) 52:120-128. DOI: 10.1111/1752-1688.12372.
# Citation for StreamCatTools:   Weber, Marc H, Hill, Ryan A., Brookes, Allen F. 2024, StreamCatTools: Tools to work with the StreamCat API within R and access the full suite of StreamCat and LakeCat metrics, https://usepa.github.io/StreamCatTools
sc_ws_metrics <- read_csv(file.path(in.dir, "StreamCat_clusterVars.csv"))  %>%
dplyr::filter(Type == "watershed") %>%
pull(Variable)
sc_ws_metrics_str <- paste(sc_ws_metrics, collapse = ",")
WS.STATE <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
state = stateAbb,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_id <- WS.STATE %>% dplyr::select(COMID) %>%
mutate(ReachLoc = "Core")
stragglers <- setdiff(NHD.STATE$COMID, WS.STATE$COMID)  # COMIDs pulled the from NHD API but not StreamCat API
n_stragglers <- stragglers %>% length()
stragglers_str <- stragglers %>% paste(collapse = ",")
message("Reading in StreamCat variables for straggler COMIDS")
message("Requires ", ceiling(n_stragglers/500), " requests")
if(ceiling(n_stragglers/500) == 0){
message("n_stragglers = 0")
} else {
sc_stragglers <-NULL
message("n_stragglers/500 > 0")
for(q in 1:ceiling(n_stragglers/500)){  # pulling in 500 COMIDs at a time to not overwhelm the server
start_ind <- ((q-1)*500) + 1
end_ind <- 500 * q
print(paste0("getting ", start_ind, ":", end_ind))
temp.comids <- stragglers[start_ind:end_ind]
temp.comids <- temp.comids[!is.na(temp.comids)] %>% paste(collapse = ",")
tryCatch({temp_sc <- StreamCatTools::sc_get_data(metric = sc_ws_metrics_str,
aoi = 'watershed',
comid = temp.comids,
showAreaSqKm = TRUE) %>%
dplyr::select(-catareasqkm, -catareasqkmrp100, -wsareasqkmrp100) %>%
rename("COMID" = "comid")
sc_stragglers <- sc_stragglers %>% bind_rows(temp_sc)
}, error = function(msg){
return(sc_stragglers)
})
}
}
sc_id <- sc_id %>% bind_rows(sc_stragglers %>% dplyr::select(COMID) %>% mutate(ReachLoc = "Buffer"))
WS.STATE <- WS.STATE %>% bind_rows(sc_stragglers)
WS.STATE.SCvars <- dplyr::left_join(NHD.STATE, WS.STATE, by = "COMID")
save(WS.STATE.SCvars, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_", state, ".rda")))
save(sc_id, file = file.path(out.dir, "NHDPlus",
paste0("NHD_SC_ID", state, ".rda")))
}
tictoc::toc(log = TRUE)
# STEP 4: Data QC ----
tictoc::tic("Perform QC of stream reach data")
# check if there are any exact duplicates and remove
n_dup <- WS.STATE.SCvars %>% group_by_all() %>% filter(n()>1) %>% nrow()
if(n_dup == 0){
message("No duplicated rows")
}
if(n_dup!= 0){
message("Removing duplicate rows")
WS.STATE.SCvars <- WS.STATE.SCvars %>% distinct()
}
# drop columns with all NAs
na_cols <- WS.STATE.SCvars %>%
st_drop_geometry() %>%
select_if(function(x) all(is.na(x))) %>%
names()
if(length(na_cols) == 0){
message("No completely empty columns")
}
if(length(na_cols) != 0){
WS.STATE.SCvars <- WS.STATE.SCvars %>% dplyr::select(-all_of(na_cols))
}
# replace negative slopes with NAs
n_neg_slope <- WS.STATE.SCvars %>% filter(SLOPE < 0) %>% nrow()
WS.STATE.SCvars <- WS.STATE.SCvars %>% mutate(SLOPE = if_else(SLOPE < 0, NA, SLOPE))
message(paste0("Replacing ", n_neg_slope, " negative slope values with NAs"))
# remove COMIDs without any StreamCat data
WS.STATE.FinalRaw <- WS.STATE.SCvars %>%
st_drop_geometry() %>%
rename("wsareasqkmws" = "wsareasqkm") %>%
filter(!(if_all(ends_with("ws"), is.na)))
n_na_row <- WS.STATE.SCvars %>% rename("wsareasqkmws" = "wsareasqkm") %>% filter(if_all(ends_with("ws"), is.na)) %>% nrow()
message(paste0("Removing ", n_na_row, " rows without StreamCat data"))
save(WS.STATE.FinalRaw, file = file.path(out.dir, "PCA",
paste0("FinalRawData", state, ".rda")))
tictoc::toc(log = TRUE)
# STEP 5: Finalize dataset ----
## Get stats ----
tictoc::tic("Get statistics & transform variables, if necessary")
if (file.exists(file.path(out.dir, "Histograms", paste0(state, "_stats.tab")))) {
WS.STATE.stats <- read.delim(file.path(out.dir, "Histograms", paste0(state, "_stats.tab")) , sep = "\t")
message("Previously saved histogram stats")
} else {
WS.STATE.stats <- WS.STATE.FinalRaw %>%
tidyr::pivot_longer(cols = !COMID, names_to = "Variable", values_to = "Value",
values_drop_na = FALSE) %>%
dplyr::group_by(Variable) %>%
dplyr::summarise(N = dplyr::n() - sum(is.na(Value))
, NumNAs = sum(is.na(Value))
, Min = min(Value, na.rm = TRUE)
, Max = max(Value, na.rm = TRUE)
, Mean = mean(Value, na.rm = TRUE)
, Median = median(Value, na.rm = TRUE)
, SD = sd(Value, na.rm = TRUE)
, Kurtosis = moments::kurtosis(Value, na.rm = TRUE)
, Skewness = moments::skewness(Value, na.rm = TRUE)
, SkewnessSq = Skewness * Skewness
, .groups = "drop_last")
write.table(WS.STATE.stats
, file.path(out.dir, "Histograms", paste0(state, "_stats.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
}
if (file.exists(file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab"))) & file.exists(file.path(out.dir, "QC", paste0(state, "_lambdas.tab")))){
WS.STATE.FinalTransf <- read.delim(file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab")), sep = "\t")
df.lambda <- read.delim(file.path(out.dir, "QC", paste0(state, "_lambdas.tab")), sep = "\t")
message("Previously saved QC files")
} else{
## Transform/scale variables ----
## Plot histograms before/after ----
cols <- setdiff(colnames(WS.STATE.FinalRaw), "COMID")
df.temp <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.temp_scaled <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.lambda <- data.frame(Variable = character(), Lambda = double())
rm_vars <- c()
for (c in seq_along(cols)) {
col <- cols[c]
print(col)
fn <- paste0(col, ".png")
fn2 <- paste0(col, "_transf.png")
# plot histogram of data
p <- ggplot2::ggplot(WS.STATE.FinalRaw, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn), p, width = 6, height = 4
, units = "in")
sk_sq <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SkewnessSq)
sd_dat <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SD)
if (as.numeric(sd_dat) != 0) {
if (grepl("PCT", col)) {         # Do not transform PCT variables
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else if (as.numeric(sk_sq)< 3){                                # Do not transform values with minimal skewness
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else {     # Box-Cox transform highly skewed variables
v_val <- WS.STATE.FinalRaw[[col]] + 1e-12
# browser()
bc <- caret::BoxCoxTrans(v_val, na.rm = TRUE)
lambda <- bc$lambda
# bc <- MASS::boxcox(lm(v_val ~ 1), plotit = FALSE) # for some reason, MASS::boxcox will not evaluate in a function
# lambda <- bc$x[which.max(bc$y)]
if (lambda != 0) {
new_v <- ((v_val ^ lambda) - 1) / lambda
} else {
msg <- paste(col, " has lambda equal zero.")
message(msg)
new_v <- log10(v_val)
}
if (all(is.na(new_v))) {
message("all new_v NA")
new_v <- WS.STATE.FinalRaw[[col]]
subtitle <- "Not transformed, centered and scaled"
} else {
subtitle <- paste0("lambda = ", round(lambda, 4), ", centered and scaled")
}
df.temp <- df.temp %>% mutate(!!col := new_v)
}
# Scale variables, this is just for visualization of the centered and scaled distribution.
# The the unscaled and uncentered data are passed to the PCA functions which scale and center the data.
new_v_scaled <- scale(new_v, center = TRUE, scale = TRUE)
df.temp_scaled <- df.temp_scaled %>% mutate(!!col := new_v_scaled)
p2 <- ggplot2::ggplot(df.temp_scaled, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::labs(subtitle = subtitle) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn2), p2
, width = 6, height = 4, units = "in")
df.lambda <- rbind(df.lambda, cbind(col, round(lambda, 4)))
}
if (as.numeric(sd_dat) == 0) {          # Do not include variables with zero variation
rm_vars <- c(rm_vars, col)
}
}
message(paste0("removed: ", paste(rm_vars, collapse = ", "), " for 0 variation"))
WS.STATE.FinalTransf <- df.temp
write.table(WS.STATE.FinalTransf, file.path(out.dir, "QC", paste0(state, "_WsTransfData.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
write.table(df.lambda, file.path(out.dir, "QC", paste0(state, "_lambdas.tab"))
, sep = "\t", col.names = TRUE, row.names = FALSE, append = FALSE)
}
cols
## Transform/scale variables ----
## Plot histograms before/after ----
cols <- setdiff(colnames(WS.STATE.FinalRaw), "COMID")
df.temp <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.temp_scaled <- dplyr::select(WS.STATE.FinalRaw, COMID)
df.lambda <- data.frame(Variable = character(), Lambda = double())
rm_vars <- c()
for (c in seq_along(cols)) {
col <- cols[c]
print(col)
fn <- paste0(col, ".png")
fn2 <- paste0(col, "_transf.png")
# plot histogram of data
p <- ggplot2::ggplot(WS.STATE.FinalRaw, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn), p, width = 6, height = 4
, units = "in")
sk_sq <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SkewnessSq)
sd_dat <- WS.STATE.stats %>%
dplyr::filter(Variable == col) %>%
dplyr::select(SD)
if (as.numeric(sd_dat) != 0) {
if (grepl("pct", col)) {         # Do not transform PCT variables
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else if (as.numeric(sk_sq)< 3){                                # Do not transform values with minimal skewness
lambda <- NA_real_
new_v <- WS.STATE.FinalRaw[[col]]
df.temp <- df.temp %>% mutate(!!col := new_v)
subtitle <- "Not transformed, centered and scaled"
}
else {     # Box-Cox transform highly skewed variables
v_val <- WS.STATE.FinalRaw[[col]] + 1e-12
# browser()
bc <- caret::BoxCoxTrans(v_val, na.rm = TRUE)
lambda <- bc$lambda
# bc <- MASS::boxcox(lm(v_val ~ 1), plotit = FALSE) # for some reason, MASS::boxcox will not evaluate in a function
# lambda <- bc$x[which.max(bc$y)]
if (lambda != 0) {
new_v <- ((v_val ^ lambda) - 1) / lambda
} else {
msg <- paste(col, " has lambda equal zero.")
message(msg)
new_v <- log10(v_val)
}
if (all(is.na(new_v))) {
message("all new_v NA")
new_v <- WS.STATE.FinalRaw[[col]]
subtitle <- "Not transformed, centered and scaled"
} else {
subtitle <- paste0("lambda = ", round(lambda, 4), ", centered and scaled")
}
df.temp <- df.temp %>% mutate(!!col := new_v)
}
# Scale variables, this is just for visualization of the centered and scaled distribution.
# The the unscaled and uncentered data are passed to the PCA functions which scale and center the data.
new_v_scaled <- scale(new_v, center = TRUE, scale = TRUE)
df.temp_scaled <- df.temp_scaled %>% mutate(!!col := new_v_scaled)
p2 <- ggplot2::ggplot(df.temp_scaled, ggplot2::aes(x = .data[[col]])) +
ggplot2::geom_histogram(bins = 500) +
ggplot2::ggtitle(paste0("Histogram of ", col, " observations")) +
ggplot2::labs(subtitle = subtitle) +
ggplot2::xlab(col) +
ggplot2::theme_bw()
ggplot2::ggsave(file.path(out.dir, "Histograms", fn2), p2
, width = 6, height = 4, units = "in")
df.lambda <- rbind(df.lambda, cbind(col, round(lambda, 4)))
}
if (as.numeric(sd_dat) == 0) {          # Do not include variables with zero variation
rm_vars <- c(rm_vars, col)
}
}
source("~/3-projects/10-castool/CASToolClusterPckg/data-raw/clusterReaches_fastclust.R")
state.vec <- c("Oregon", "Washington", "Delaware")
hash <- system("git rev-parse --short=8 HEAD", intern = TRUE)
df <- data.frame(state = character(), numclust = character(), fn = character())
for(i in 1:length(state.vec)){
current_state <- state.vec[i]
print(current_state)
temp <- clusterReaches(state = current_state, pct_var = 60, minCOMIDsCluster = 0.2, user_numclust = NULL, commit_hash = hash)
df <- df %>%
dplyr::filter(state != current_state) %>%
bind_rows(temp)
rm(list = setdiff(ls(), c("state.vec", "hash", "df", "clusterReaches")))
}
write.csv(df, file.path("inst", "extdata", "pick_list.csv"), row.names = FALSE)
load("~/3-projects/10-castool/CASToolClusterPckg/data/Washington_ClusterAssignments_20250714115415_4_0f5f4dfb.rda")
View(Washington_ClusterAssignments_20250714115415_4_0f5f4dfb)
